# Heuristic Router: Speed > Smarts

## The Problem

**The Naive Approach:**
> "Let's use a small LLM (like GPT-3.5) to classify the user's intent, and then route it to the right model."

**The Engineering Reality:**
This is **"Model-on-Model" overhead**. Even a small LLM takes 500ms+ to think. You are adding latency just to decide where to send the traffic. We need to be **Fast, even if we are occasionally Wrong**.

### Example of the Problem

```
User Query: "Hi"

‚ùå With AI Classifier:
  1. Call GPT-3.5 to classify (500ms, $0.01)
  2. Get result: "greeting"
  3. Route to appropriate handler
  Total: 500ms latency + $0.01 cost

‚úÖ With Heuristic Router:
  1. Pattern match "Hi" ‚Üí CANNED response
  Total: <1ms latency + $0.00 cost
```

**Result:** The AI classifier costs **$3,650/year** for 1000 daily greetings, while the heuristic router costs **$0**.

## The Solution

### Philosophy: Deterministic Heuristics, Not AI Classifiers

We can solve **80% of routing** with simple logic that takes **0ms**:

- **Rule 1:** Is the query length < 50 characters? ‚Üí Send to **Fast Model** (GPT-4o-mini)
- **Rule 2:** Does it contain keywords like "Summary", "Analyze", "Compare"? ‚Üí Send to **Smart Model** (GPT-4o)
- **Rule 3:** Is it a greeting ("Hi", "Thanks")? ‚Üí Send to **Canned Response** (Zero Cost)

**The Goal:**
- ‚úÖ Instant response time for trivial stuff
- ‚úÖ Preserve the "Big Brain" budget for hard stuff
- ‚úÖ Accept 15% accuracy loss for 500x speed improvement

## Model Tiers

### üéØ CANNED (Zero Cost, 0ms latency)

**When to use:**
- Greetings: "Hi", "Hello", "Hey"
- Acknowledgments: "Thanks", "Got it", "Ok"
- Farewells: "Bye", "Goodbye"

**Benefits:**
- No API call required
- Instant response (< 1ms)
- Zero cost ($0.00 per request)

**Example:**
```python
router = HeuristicRouter()
decision = router.route("Hi")

print(decision.model_tier)  # ModelTier.CANNED
print(decision.estimated_cost)  # "zero"

response = router.get_canned_response("Hi")
print(response)  # "Hello! How can I assist you today!"
```

### ‚ö° FAST (Low Cost, ~200ms latency)

**When to use:**
- Short queries (< 50 chars)
- Simple questions: "What is X?", "How to Y?"
- Quick lookups: "Status check", "Show logs"

**Benefits:**
- 100x cheaper than GPT-4o
- Fast response (~200ms)
- Good enough for 80% of queries

**Cost:** ~$0.0001 per request

**Example:**
```python
decision = router.route("What is Python?")

print(decision.model_tier)  # ModelTier.FAST
print(decision.suggested_model)  # "gpt-4o-mini"
print(decision.estimated_cost)  # "low"
```

### üß† SMART (High Cost, ~500ms+ latency)

**When to use:**
- Complex tasks: "Summarize", "Analyze", "Compare"
- Long queries (‚â• 50 chars)
- Tasks requiring reasoning

**Keywords that trigger SMART:**
- summarize, summary
- analyze, analysis
- compare, comparison
- evaluate, assessment
- review, critique
- comprehensive, thorough
- deep dive, investigate

**Benefits:**
- Best quality for complex tasks
- Worth the cost for hard problems

**Cost:** ~$0.01 per request

**Example:**
```python
decision = router.route("Summarize this document")

print(decision.model_tier)  # ModelTier.SMART
print(decision.suggested_model)  # "gpt-4o"
print(decision.matched_keywords)  # ["summarize"]
print(decision.estimated_cost)  # "high"
```

## Routing Rules (Priority Order)

The router evaluates rules in this order:

### Priority 1: Greetings ‚Üí CANNED

```python
# Pattern: exact match or short phrase (‚â§3 words)
"Hi" ‚Üí CANNED
"Thanks" ‚Üí CANNED
"Hello there" ‚Üí CANNED
"Thank you" ‚Üí CANNED
```

### Priority 2: Smart Keywords ‚Üí SMART

```python
# Contains any smart keyword
"Summarize this" ‚Üí SMART
"Analyze performance" ‚Üí SMART
"Compare approaches" ‚Üí SMART
```

### Priority 3: Query Length ‚Üí FAST/SMART

```python
# Short query (< 50 chars, no keywords)
"What is Python?" ‚Üí FAST

# Long query (‚â• 50 chars, no keywords)
"Can you tell me more about the implementation..." ‚Üí SMART
```

## Usage

### Basic Usage

```python
from caas.routing import HeuristicRouter

# Initialize router
router = HeuristicRouter()

# Route a query
decision = router.route("Summarize this document")

print(f"Tier: {decision.model_tier}")
print(f"Model: {decision.suggested_model}")
print(f"Cost: {decision.estimated_cost}")
print(f"Confidence: {decision.confidence}")
print(f"Reason: {decision.reason}")
```

### Custom Configuration

```python
# Custom query length threshold
router = HeuristicRouter(
    short_query_threshold=30,  # Default: 50
    enable_canned_responses=True  # Default: True
)

# This query (35 chars) now routes to SMART instead of FAST
decision = router.route("What is the status of the build?")
print(decision.model_tier)  # ModelTier.SMART
```

### Canned Responses

```python
router = HeuristicRouter()

# Get canned response for greetings
response = router.get_canned_response("Hi")
print(response)  # "Hello! How can I assist you today!"

response = router.get_canned_response("Thanks")
print(response)  # "You're welcome! Let me know if you need anything else."
```

### API Endpoint

```bash
curl -X POST "http://localhost:8000/route" \
  -H "Content-Type: application/json" \
  -d '{"query": "Summarize this document"}'
```

**Response:**
```json
{
  "model_tier": "smart",
  "reason": "Complex task keywords detected: summarize",
  "confidence": 0.85,
  "query_length": 23,
  "matched_keywords": ["summarize"],
  "suggested_model": "gpt-4o",
  "estimated_cost": "high"
}
```

**For greetings:**
```bash
curl -X POST "http://localhost:8000/route" \
  -H "Content-Type: application/json" \
  -d '{"query": "Hi"}'
```

**Response:**
```json
{
  "model_tier": "canned",
  "reason": "Greeting detected - using canned response for zero cost",
  "confidence": 0.95,
  "query_length": 2,
  "matched_keywords": ["hi"],
  "suggested_model": "canned_response",
  "estimated_cost": "zero",
  "canned_response": "Hello! How can I assist you today!"
}
```

## Performance Metrics

### Speed Comparison

| Method | Routing Time | Speedup |
|--------|-------------|---------|
| Heuristic Router | < 1ms | 1x (baseline) |
| AI Classifier (GPT-3.5) | ~500ms | **500x slower** |
| AI Classifier (GPT-4o-mini) | ~300ms | **300x slower** |

### Cost Comparison (1000 requests/day)

#### With AI Classifier
```
Routing cost: 1000 √ó $0.01 = $10/day
Actual AI cost: $50/day
Total: $60/day = $21,900/year
```

#### With Heuristic Router
```
Routing cost: $0/day (deterministic)
Breakdown:
  - Greetings (30%): 300 √ó $0.00 = $0/day
  - Fast queries (50%): 500 √ó $0.0001 = $0.05/day
  - Smart queries (20%): 200 √ó $0.01 = $2.00/day
Total: $2.05/day = $748/year

SAVINGS: $21,152/year (96.6% cost reduction)
```

### Accuracy vs Speed Trade-off

| Method | Accuracy | Latency | Value |
|--------|----------|---------|-------|
| AI Classifier | ~95% | 500ms | Slow but accurate |
| Heuristic Router | ~80% | <1ms | **Fast even if occasionally wrong** ‚úÖ |

**Key Insight:** For 80% of queries, instant routing is more valuable than perfect routing.

## Decision Model

Every routing decision includes:

```python
@dataclass
class RoutingDecision:
    model_tier: ModelTier  # CANNED, FAST, or SMART
    reason: str  # Why this tier was chosen
    confidence: float  # 0.0 to 1.0
    query_length: int  # Length of the query
    matched_keywords: List[str]  # Keywords that triggered decision
    suggested_model: str  # e.g., "gpt-4o-mini", "gpt-4o"
    estimated_cost: str  # "zero", "low", "medium", "high"
```

**Confidence Levels:**
- **CANNED (0.95):** Very confident - exact greeting match
- **SMART with keywords (0.85):** Confident - clear keyword match
- **FAST short query (0.80):** Good confidence - length-based
- **SMART long query (0.70):** Moderate confidence - safe default

## Testing

Run comprehensive tests:

```bash
# Unit tests
python test_heuristic_router.py

# Demo
python demo_heuristic_router.py
```

**Test Coverage:**
- ‚úÖ Rule 1: Short queries ‚Üí Fast model
- ‚úÖ Rule 2: Smart keywords ‚Üí Smart model
- ‚úÖ Rule 3: Greetings ‚Üí Canned responses
- ‚úÖ Priority order (greetings override keywords)
- ‚úÖ Edge cases (empty query, boundary conditions)
- ‚úÖ Case insensitivity
- ‚úÖ Custom thresholds
- ‚úÖ Confidence scores

## Limitations & Trade-offs

### What the Heuristic Router Does Well
- ‚úÖ Instant routing decisions (< 1ms)
- ‚úÖ Zero cost for greetings
- ‚úÖ Simple, predictable behavior
- ‚úÖ Easy to debug and customize
- ‚úÖ No API dependencies

### What It Doesn't Handle
- ‚ùå Ambiguous queries (routes conservatively to SMART)
- ‚ùå Context-dependent routing (treats each query independently)
- ‚ùå Learning from feedback (static rules)
- ‚ùå Multi-language support (English keywords only)

### When to Use AI Classifier Instead
- When routing accuracy > 95% is critical
- When handling multi-language queries
- When context from previous queries matters
- When you can afford 500ms+ routing latency

## Design Philosophy

### Core Principles

1. **Speed > Smarts**
   - Fast even if occasionally wrong > Slow but always right
   - For most queries, instant routing beats perfect routing

2. **Simple > Complex**
   - Deterministic heuristics > Machine learning
   - Easy to understand, debug, and modify

3. **Cost-Conscious**
   - Zero cost for common patterns (greetings)
   - Cheap model for simple queries
   - Expensive model only when needed

4. **Safe Defaults**
   - When uncertain, route to SMART (better safe than sorry)
   - Greetings have highest priority (zero cost)

### The 80/20 Rule

The heuristic router targets the **80% of queries** that can be routed with simple rules:
- 30% are greetings ‚Üí CANNED (zero cost)
- 50% are simple queries ‚Üí FAST (low cost)
- 20% are complex ‚Üí SMART (high cost)

This achieves **80% accuracy** with **0ms latency** and **massive cost savings**.

## Real-World Impact

### Example: Customer Support Chatbot

**Before (AI Classifier):**
- 1000 daily conversations
- Each conversation starts with greeting
- Greeting classification: 1000 √ó $0.01 = $10/day
- Annual cost: $3,650 just for routing greetings

**After (Heuristic Router):**
- Same 1000 daily conversations
- Greetings routed via pattern matching
- Greeting cost: $0/day
- Annual savings: $3,650

**Additional Benefits:**
- Instant responses for greetings (< 1ms vs 500ms)
- Better user experience
- Lower infrastructure load

### Example: AI Coding Assistant

**Query Distribution:**
- 40% greetings/acknowledgments ‚Üí CANNED ($0)
- 40% quick questions ‚Üí FAST ($0.0001 each)
- 20% complex tasks ‚Üí SMART ($0.01 each)

**Cost Comparison (10,000 queries/day):**

| Method | Daily Cost | Annual Cost |
|--------|-----------|-------------|
| All queries with GPT-4o | $100 | $36,500 |
| All queries with GPT-3.5 | $50 | $18,250 |
| With AI Classifier + GPT-4o | $120 | $43,800 |
| **With Heuristic Router** | **$20** | **$7,300** |

**Savings: $36,500/year (83% reduction)**

## Summary

The Heuristic Router solves the "Model-on-Model overhead" problem by:

1. ‚ö° **Eliminating routing latency** (500ms ‚Üí <1ms)
2. üí∞ **Reducing costs** (up to 96% savings)
3. üéØ **Maintaining quality** (~80% routing accuracy)
4. üöÄ **Improving UX** (instant responses for common queries)

**Philosophy:** "Fast even if occasionally wrong" > "Slow but always right"

**Result:** A production-ready router that handles 80% of queries with instant, zero-cost decisions, while preserving the "Big Brain" budget for the 20% that truly need it.

---

**Ready to use?**
```python
from caas.routing import HeuristicRouter

router = HeuristicRouter()
decision = router.route(your_query)
```
