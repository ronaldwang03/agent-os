# GAIA Benchmark - The "Laziness" Stress Test (Competence)

## Overview

This experiment validates the **Completeness Auditor** component by testing the agent's resilience against vague queries where data exists but requires deeper search.

## Hypothesis

**Standard GPT-4o** will give up with "File not found" or "No data available" on vague queries, even when data exists.

**Our Self-Correcting Kernel** with Completeness Auditor will:
1. Detect the "give-up signal"
2. Trigger Shadow Teacher (o1-preview) verification
3. Catch laziness when teacher finds data
4. Auto-correct with competence patch

## The Setup

### Dataset: 50 Vague Queries

Examples where the actual resource has a different name/location:

1. **Query:** "Find the Q3 report"
   - **Actual File:** `2025-Q3-Final.pdf` (in archived folder)
   - **Agent Challenge:** Needs to check archives, not just recent

2. **Query:** "Get logs for error 500"
   - **Actual Location:** `/var/log/archive/2024-01/error_500_logs.json`
   - **Agent Challenge:** Needs to check archived partitions

3. **Query:** "Show project Alpha status"
   - **Actual Truth:** Project Alpha was renamed to "Project_Artemis"
   - **Agent Challenge:** Needs to check rename history

4. **Query:** "Find customer feedback from last quarter"
   - **Actual Location:** Database table `customer_surveys_q4_2024`
   - **Agent Challenge:** Needs to translate "last quarter" to table name

5. **Query:** "Get the latest deployment config"
   - **Actual File:** `deploy_config_v2.3.1_prod.yaml`
   - **Agent Challenge:** Needs to check versioned configs

### Test Categories

- **Archived Resources** (20 queries): Data exists in archives
- **Renamed Entities** (15 queries): Resources were renamed
- **Time-Based Confusion** (10 queries): "Recent", "latest", "last week"
- **Synonym Issues** (5 queries): Different terminology for same thing

## The Baseline

Run **standard GPT-4o** on all 50 queries without Completeness Auditor.

**Expected Baseline Results:**
- ~30-40 "Not found" responses (false negatives)
- Agent gives up without exhaustive search
- No self-correction mechanism

**Metric:** Count how many times it says "File not found" or equivalent.

## The Test

Run our **Self-Correcting Kernel** with:
1. Standard agent attempts the query
2. Completeness Auditor detects give-up signal
3. Shadow Teacher (o1-preview) re-attempts with enhanced context
4. If teacher succeeds, generate competence patch
5. Apply patch to prevent future laziness

## Metrics

### Primary Metric: Correction Rate

**Formula:** 
```
Correction Rate = (Patches Applied / Give-Up Signals Detected) Ã— 100%
```

**Target:** â‰¥70% correction rate

**Example:**
- 40 give-up signals detected (agent said "not found")
- Shadow Teacher found data in 28 cases
- Competence patches applied: 28
- **Correction Rate: 70%**

### Secondary Metrics

1. **Audit Efficiency:** % of interactions triggering audit
   - **Target:** 5-10% (only on give-up signals)
   
2. **False Positive Rate:** Teacher also couldn't find data
   - **Target:** <20% (teacher confirms agent was correct)

3. **Patch Quality:** After patching, does agent succeed on similar queries?
   - **Target:** â‰¥80% success rate post-patch

## Why This Wins

### Value Proposition

**This proves your agent tries harder than a standard OpenAI agent.**

- **Standard Agent:** Gives up on first failure
- **Our Agent:** Has a "second reviewer" (Shadow Teacher) that catches laziness
- **Result:** Higher success rate on real-world vague queries

### Business Impact

- **Customer Support:** Fewer "I couldn't find that" responses
- **Data Analysis:** More thorough data exploration
- **Documentation Search:** Better results on fuzzy queries

## Running the Experiment

### Prerequisites

```bash
pip install -r requirements.txt
```

### Run Baseline (Standard Agent)

```bash
cd experiments/gaia_benchmark
python run_baseline.py --queries dataset/vague_queries.json --output results/baseline.jsonl
```

### Run With Completeness Auditor

```bash
python run_with_auditor.py --queries dataset/vague_queries.json --output results/with_auditor.jsonl
```

### Analyze Results

```bash
python analyze_results.py --baseline results/baseline.jsonl --auditor results/with_auditor.jsonl
```

**Output:**
```
=== GAIA Benchmark Results ===

Baseline (Standard GPT-4o):
  - Total queries: 50
  - "Not found" responses: 37
  - Success rate: 26%

With Completeness Auditor:
  - Total queries: 50
  - Give-up signals: 37
  - Audits triggered: 37 (5% of total agent interactions)
  - Teacher found data: 28
  - Correction rate: 75.7%
  - Success rate post-patch: 82%

âœ¨ Improvement: +56% success rate
```

## Expected Outcomes

### Experiment A Success Criteria

âœ… **Correction Rate â‰¥ 70%**: Most laziness cases are caught and fixed

âœ… **Audit Efficiency < 10%**: Only selective auditing (not expensive)

âœ… **Post-Patch Success â‰¥ 80%**: Patches actually improve agent behavior

### Publishing Results

**The Paper:** "Differential Auditing: Detecting and Correcting Agent Laziness via Shadow Teacher Verification"

**The Demo:** Live comparison showing standard agent vs. self-correcting agent on the same vague query

## Dataset Structure

```json
{
  "queries": [
    {
      "id": "q001",
      "query": "Find the Q3 report",
      "category": "archived_resource",
      "ground_truth": {
        "exists": true,
        "location": "archive/2025-Q3-Final.pdf",
        "requires": ["check_archives"]
      },
      "expected_agent_behavior": "give_up",
      "expected_teacher_behavior": "find_it"
    }
  ]
}
```

## Future Enhancements

1. **Real GAIA Dataset:** Integrate with actual GAIA benchmark tasks
2. **Multi-Domain:** Test across different domains (code, medical, legal)
3. **Progressive Difficulty:** Rank queries by vagueness level
4. **Comparative Analysis:** Compare with other correction mechanisms

---

**Status:** ðŸš§ Setup in progress
**Next Steps:** Create dataset, implement runners, run baseline
