\begin{thebibliography}{16}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anonymous(2026)]{anonymous2026companion}
Anonymous.
\newblock [anonymized]: Companion paper on runtime policy adaptation.
\newblock \emph{Under review}, 2026.

\bibitem[Authors(2024{\natexlab{a}})]{safety2024framework}
Various Authors.
\newblock A safety framework for real-world agentic systems.
\newblock \emph{arXiv preprint arXiv:2511.21990}, 2024{\natexlab{a}}.

\bibitem[Authors(2024{\natexlab{b}})]{wildguard2024}
Various Authors.
\newblock {WildGuard}: Open one-stop moderation tools for safety risks,
  jailbreaks, and refusals of {LLMs}.
\newblock \emph{arXiv preprint arXiv:2406.18495}, 2024{\natexlab{b}}.

\bibitem[Authors(2025)]{maestro2025}
Various Authors.
\newblock {MAESTRO}: Multi-agent system evaluation and testing for reliable
  operations.
\newblock \emph{arXiv preprint arXiv:2503.03813}, 2025.

\bibitem[Chase(2022)]{langchain2022}
Harrison Chase.
\newblock {LangChain}: Building applications with {LLMs} through composability.
\newblock \url{https://python.langchain.com/}, 2022.

\bibitem[{CrewAI}(2024)]{crewai2024}
{CrewAI}.
\newblock {CrewAI}: Framework for orchestrating role-playing {AI} agents.
\newblock \url{https://docs.crewai.com/}, 2024.

\bibitem[{Deloitte}(2025)]{deloitte2025orchestration}
{Deloitte}.
\newblock Unlocking exponential value with {AI} agent orchestration.
\newblock \url{https://www2.deloitte.com/}, 2025.

\bibitem[Greshake et~al.(2023)Greshake, Abdelnabi, Mishra, Endres, Holz, and
  Fritz]{greshake2023not}
Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten
  Holz, and Mario Fritz.
\newblock Not what you've signed up for: Compromising real-world
  {LLM}-integrated applications with indirect prompt injection.
\newblock \emph{arXiv preprint arXiv:2302.12173}, 2023.

\bibitem[{Guardrails AI}(2023)]{guardrailsai2023}
{Guardrails AI}.
\newblock Guardrails {AI}: Adding guardrails to large language models.
\newblock \url{https://www.guardrailsai.com/}, 2023.

\bibitem[Inan et~al.(2023)Inan, Upasani, Chi, et~al.]{inan2023llamaguard}
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, et~al.
\newblock Llama guard: {LLM}-based input-output safeguard for human-{AI}
  conversations.
\newblock \emph{arXiv preprint arXiv:2312.06674}, 2023.

\bibitem[{Microsoft Research}(2023)]{autogen2023}
{Microsoft Research}.
\newblock {AutoGen}: Enabling next-gen {LLM} applications via multi-agent
  conversation.
\newblock \url{https://microsoft.github.io/autogen/}, 2023.

\bibitem[{NIST}(2014)]{nist2014abac}
{NIST}.
\newblock Guide to attribute based access control ({ABAC}) definition and
  considerations.
\newblock Technical Report SP 800-162, National Institute of Standards and
  Technology, 2014.

\bibitem[{NVIDIA}(2023)]{nvidia2023nemo}
{NVIDIA}.
\newblock {NeMo} guardrails: A toolkit for controllable and safe {LLM}
  applications.
\newblock \url{https://github.com/NVIDIA/NeMo-Guardrails}, 2023.

\bibitem[{Significant-Gravitas}(2023)]{autogpt2023}
{Significant-Gravitas}.
\newblock {AutoGPT}: An autonomous {GPT-4} experiment.
\newblock \url{https://github.com/Significant-Gravitas/AutoGPT}, 2023.

\bibitem[Wei et~al.(2023)Wei, Haghtalab, and Steinhardt]{wei2023jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does {LLM} safety training fail?
\newblock \emph{arXiv preprint arXiv:2307.02483}, 2023.

\bibitem[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal}
Andy Zou, Zifan Wang, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language
  models.
\newblock \emph{arXiv preprint arXiv:2307.15043}, 2023.

\end{thebibliography}
